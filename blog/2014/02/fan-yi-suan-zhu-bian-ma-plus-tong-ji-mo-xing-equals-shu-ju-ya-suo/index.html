
<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
	<meta charset="utf-8">
	<title>[翻译]算术编码+统计模型=数据压缩  | Disp. Nil.</title>

	<meta name="author" content="zhzhzoo">

<meta name="description" content="Life and Thoughts of zhzhzoo"> <meta name="keywords" content="">

	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Disp. Nil." type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	

</head>



<body>
	<header id="header" class="inner"><h1><a href="/">Disp. Nil.</a></h1>
<span class="tagline">Disciplined Nihility</span>
<!-- <nav id="main-nav"><ul>
	<li><a href="/about">About</a></li>
	<li><a href="/archives">Archives</a></li>
	<li><a href="/contact">Contact</a></li>
</ul>
</nav> -->
<!-- <nav id="mobile-nav"> -->
<!-- 	<div class="alignleft menu"> -->
<!-- 		<a class="button">Menu</a> -->
<!-- 		<div class="container"><ul>
	<li><a href="/about">About</a></li>
	<li><a href="/archives">Archives</a></li>
	<li><a href="/contact">Contact</a></li>
</ul>
</div> -->
<!-- 	</div> -->
<!-- </nav> -->


</header>

	<div id="content" class="inner"><article class="post">
	<header>
		<h2 class="title">[翻译]算术编码+统计模型=数据压缩</h2>
		<div class="meta date">








  


<time datetime="2014-02-18T22:00:00+08:00" pubdate data-updated="true">Feb 18<span>th</span>, 2014</time></div>
	</header>
	<div class="entry-content"><p>[翻译]算术编码+统计模型=数据压缩</p>

<p>当今几乎全部的数据压缩方法无外乎这两类：字典方法和统计方法。当设备处理能力有限时，字典方法看来更为流行。不过，通过结合算术编码和强大的模型，统计方法事实上可以给出更好的效果。本文分两段，讨论怎么把算术编码和几种不同的统计模型结合起来，以达到不凡的压缩比。第一段详述算术编码如何工作。第二段讲如何开发有效的统计模型，从而和一个算术编码器一起可以产生一个高效的压缩程序。</p>

<h2>两种方法：情意缱绻</h2>

<p>一般来说，数据压缩是这样的：输入些符号，处理它们，然后把它们编码，写进压缩文件。为写作方便，本文中符号就是字节，不过符号可以是随意的什么东西，像像素，80位浮点数，EBCDIC 字符都行。压缩必须是有意义的，即压缩后的文件能够变回压缩前。当然，要是压缩后的文件比压缩前小，那是再好不过的。</p>

<p>字典方法把输入文本中的符号组换成定长编码。一个著名的例子是 LZW 压缩算法。（参见
89年10月刊 <em>Dr. Dobb&rsquo;s Journal</em>）LZW把理论上不限长度的字符串编成通常在 9 到 16 位间的编码。</p>

<p>数据压缩的统计方法走了完全不同的路。它们一次编码一个符号。符号编成可变长度的代码。每个符号对应的编码长度依符号出现的概率或频率而变。低频符号多用几位，高频符号少用几位。</p>

<p>生产生活中，统计和字典方法的界线往往不那么分明。有些方法难以明确归类，有些方法混合了这两者的特征。不过，下面讲的是用算术编码来实现的纯统计压缩方法。</p>

<h2>Huffman 编码：英雄老矣</h2>

<p>仅仅准确地算出来数据流中字符的出现概率还不够，我们还需要一种有效利用这一信息的编码方式。基于概率的编码方法中，大概最著名的就是 Huffman 编码。D. A. Huffman
在 1952 年发了篇论文，描述了一种给符号出现概率建编码表的方法。当编码长度固定时，Huffman 编码表保证产生输入符号流可能的编码中，位数最小的编码。Huffman
把它们称作最小冗余编码，但现在一般叫它们 Huffman 编码。其它定长编码系统，比如
Shannon-Fano 编码，被 Huffman 证出不最优了。</p>

<p>Huffman 编码给每个字符分配一个输出编码，输出编码可短至 1 位，也可能远长于输入符号的编码，这个长度严格取决于符号出现的概率。设 <code>p</code> 为某个字符的出现概率，则最优方案中这个字符的编码长度是 <code>log_2(1/p)</code>。因而，若某个字符出现的概率为
<code>1/256</code>，比如随机字节流的情况，它的最优编码长度是 <code>log_2 256</code> 即 <code>8</code>。若频率上升至 <code>1/2</code>，最优编码长度就降至 <code>1</code>。</p>

<p>这个方法的问题是，Huffman 编码长度必须是整数位长。比如，若某字符有 <code>1/3</code> 的概率出现，则该字符的最优编码应长 <code>1.6</code>。但 Huffman 方法会给它 <code>1</code> 或 <code>2</code> 位，无论哪个都会导致生成比理论最优长的压缩结果。</p>

<p>如果一个字符出现概率非常高，那这种不最优就成了个大问题。比如有种统计方法，给某个字符算了 <code>90%</code> 的概率，那么最优编码长度是 <code>0.15</code> 位。但 Huffman 编码可能会给它编 <code>1</code> 位，6 倍于必须。</p>

<h2>自适应方法</h2>

<p>当试着做自适应压缩的时候，Huffman 编码的第二个问题出现了。做非自适应压缩的时侯，压缩程序首先扫描数据，获得统计信息。之后依统计信息编码数据，而统计信息在编码过程中不发生改变。</p>

<p>解码者要解解码，首先需要一份统计信息。一般来说编码者会在压缩后消息前加上一张统计表，这样解码者就可以在解码前读出统计信息。这么做显然会给压缩后的消息增加一定负担。</p>

<p>当使用简单模型压缩数据时，编码表往往很小。比如，只给每个字符算一个频率，这个信息就可以很精确地存在 <code>256</code> 字节里。除非信息本身就短，这样做并不会显著增长消息。然而，为了提高压缩比，统计模型会变大。如果统计信息太大了，压缩比的优化会被统计信息的附加长度抹杀。</p>

<p>为解决此问题，自适应数据压缩应运而生。在自适应数据压缩中，编码者和解码者的统计模型在开始时是相同的。它们俩每次都处理一个字符，并在读入这个字符后更新统计模型。这和大部分基于字典的方法如 LZW 的工作方式很像。自适应方法可能会损失一些效率，因为开始时模型不是最优的，不过一般来说这要比附上统计信息好。</p>

<p>用 Huffman 编码来做自适应压缩的问题是，重建 Huffman 树代价很高。每读入一个字符，
Huffman 树都得调整一次，这样自适应方法才够高效。Huffman 编码出来 20 年后才发明自适应 Huffman 编码。就算现在，最好的 Huffman 编码算法还是很费时间和内存。</p>

<h2>算术编码：怎么做的</h2>

<p>算术编码是个令人起敬的 Huffman 编码候选人，不过它的成功出现还只是近十年的事情。算术编码完全绕过了用一个特定编码换每个输入符号的想法。反之，它考虑这个输入符号流把它换成一个浮点数输出。待编码的消息越长越繁，输出的数要用的位数就越多。而在定长寄存器的电脑上找到实现方法已经是最近的事了。</p>

<p>算术编码的输出是一个数，小于 1 大于等于 0。这个数可以唯一解码成编码前的符号流。要把输出的数构造出来，待编码的每个符号都要给一个概率。比如，我要编码一个随机信息
&ldquo;BILL GATES&#8221;，我会构造如下的概率分布：</p>

<pre><code>字符    概率
----    ----
空格    1/10
 A      1/10
 B      1/10
 E      1/10
 G      1/10
 I      1/10
 L      2/10
 S      1/10
 T      1/10
</code></pre>

<p>一旦字符的概率确定，每个符号需要分配一个概率数轴上的区间，所谓概率数轴就是区间
<code>[0,1)</code>。哪个字符占这区间的哪段并不重要，只要编码者和解码者相同就行。对这个九个字符的符号集，情况是这样的：</p>

<pre><code>字符    概率        区间
----    ----    -----------
空格    1/10    0.00 - 0.10
 A      1/10    0.10 - 0.20
 B      1/10    0.20 - 0.30
 E      1/10    0.30 - 0.40
 G      1/10    0.40 - 0.50
 I      1/10    0.50 - 0.60
 L      2/10    0.60 - 0.80
 S      1/10    0.80 - 0.90
 T      1/10    0.90 - 1.00
</code></pre>

<p>每个字符拥有 <code>[0,1)</code> 区间与它出现概率相对应的一部分。还要注意每个区间是左闭右开的，比如 &rsquo;T&#8217; 的就是<code>[0.90, 1.00)</code>。</p>

<p>算术编码后消息的最高位部分取决于第一个符号。编码 &ldquo;BILL GATES&rdquo; 时，第一个符号是
&lsquo;B&#8217;。为了让解码出的第一个符号对，最终的消息要大于 <code>0.20</code> 但小于 <code>0.30</code>。我们编码出这个实数的过程就是追踪这个数落在哪个区间里。第一个字符编码后，这个区间是
<code>[0.20, 0.30)</code>。</p>

<p>第一个字符编码后，我们知道最终输出的实数已为一区间所限。之后的编码过程就是每个新来的符号细化区间限制的过程。下一个编码的字符，&#8217;I&#8217;，拥有区间 <code>[0.50, 0.60)</code>。假如它是第一个字符，那输出数就是这个区间。但 &lsquo;I&rsquo; 是第二个字符。所以我们会讲，&#8217;I&#8217; 拥有
<code>[0.20, 0.30)</code> 中对应 <code>[0.50, 0.60)</code> 的那部分。即新编码的数会落在当前区间的 50% 和
60 % 之间。算出来，最后的数就会落在 <code>[0.25, 0.26)</code>。</p>

<p>对任何长度的信息，计算输出数的算法如下：</p>

<pre><code>Set low to 0.0
Set high to 1.0
While there are still input symbols do
    get an input symbol
    code_range - high - low.
    high - low + range*high_range(symbol)
    low - low + range*low_range(symbol)
End of While
output low
</code></pre>

<p>在我们的信息上这样一直做下去，直到算法终止，得到：</p>

<pre><code>新字符      下界             上界
------   ----------       ----------
         0.0              1.0
  B      0.2              0.3
  I      0.25             0.26
  L      0.256            0.258
  L      0.2572           0.2576
 空格    0.25720          0.25724
  G      0.257216         0.257220
  A      0.2572164        0.2572168
  T      0.25721676       0.2572168
  E      0.257216772      0.257216776
  S      0.2572167752     0.2572167756
</code></pre>

<p>所以用我们的方法，最后的下界，<code>0.2572167752</code> 唯一编码了消息 &ldquo;BILL GATES&#8221;。</p>

<p>有了编码方法，去看怎么解码就相对容易了。先看编码后信息落在哪个字符的区间里，找到第一个字符。因为数 <code>0.2572167752</code> 落在 <code>[0.2, 0.3)</code>，我们知道第一个字符肯定是
&lsquo;B&#8217;。现在我们需要从编码出的数里去掉 &#8216;B&#8217;。而我们又知道 &#8216;B&rsquo; 对应的区间， 我们可以做与编码时相反的过程来去掉 &lsquo;B&#8217;。首先，我们从数里减去区间下界，得 <code>0.0572167752</code>。接下来除以 <code>B</code> 的区间长度 <code>0.1</code>。得 <code>0.572167752</code>。随后我们就可以计算这个数落在哪个区间里了，这个区间就是下一个字符 <code>I</code> 所对的区间。</p>

<p>解码一个数的算法如下：</p>

<pre><code>get encoded number 
Do 
    find symbol whose range straddles the encoded number 
    output the symbol 
    range - symbol low value - symbol high value 
    subtract symbol low value from encoded number 
    divide encoded number by range 
until no more symbols 
</code></pre>

<p>要注意到这里我们只是随便说了句没有更多的符号了，忽略了怎么判断。其实可以在最后编码一个特殊 <code>EOF</code> 符号，或者在编码后的消息里加上数据的长度。</p>

<p>解压 &ldquo;BILL GATES&rdquo; 的算法大概执行如下：</p>

<pre><code>   编码出的数      输出符号      下界   上界   长度
--------------  ------------    ----   ----   -----
0.2572167752          B         0.2    0.3    0.1
0.572167752           I         0.5    0.6    0.1
0.72167752            L         0.6    0.8    0.2
0.6083876             L         0.6    0.8    0.2
0.041938             空格       0.0    0.1    0.1
0.41938               G         0.4    0.5    0.1
0.1938                A         0.2    0.3    0.1
0.938                 T         0.9    1.0    0.1
0.38                  E         0.3    0.4    0.1
0.8                   S         0.8    0.9    0.1
0.0
</code></pre>

<p>总之，编码的过程就是根据每个新来的符号细化输出数可能落进的区间罢了。新区间长度与之前为此符号定的概率成正比。解压缩是逆过程，区间长度依正比例随着每个符号被解出来而不断放大。</p>

<h2>生产生活中</h2>

<p>用算术编码编码和解码一串符号并不十分复杂。但是第一眼看上去，这根本不可行。大多数电脑仅支持 80 位左右的浮点数。难道这就意味着你每编码 10 或 15 个符号就得从头再来吗？还是说你得买个浮点数处理器？浮点数格式不同的电脑可以用算术编码交换信息吗？</p>

<p>事实上，算术编码最好是用标准 16 位和 32 位整数来实现。不需要浮点数，用了效果也不会更好。替代它的是一种增量转换方法，这里用定长整数变量记录状态，新来的位从低位加进来，从高位移走，最后形成一个数，可以有电脑存储器那么多位长。</p>

<p>在上一节，我讲了算法是怎么追踪输出数可能落在的区间里。当算法开始执行时，下界设成 <code>0.0</code>，上界设成 <code>1.0</code>。为了用整数算，第一个简化是把 <code>1.0</code> 写成 <code>0.999…</code>，或者在二进制里是 <code>.111…</code>。</p>

<p>为了把这些数存在整型寄存器里，我们首先把它们对齐，让想象中的小数点位于每个字的左边。然后我们把起始的上下界装进去，把浮点数装进整数里，肯定装不下，能装多少位就装多少位。我的实现用 16 位无符号整数，所以上界的初始值是 <code>0xFFFF</code>，下界是 <code>0</code>。我们知道上界是 <code>FF</code> 的无限循环，下界是 <code>0</code> 的无限循环，所以我们可以在要用它们的时候把这些位移进来，而并无大碍。</p>

<p>要是你想象在一个 5 位数十进制寄存器里做我们的 &ldquo;BILL GATES&rdquo; 例子，初始情况是这样的：</p>

<pre><code>HIGH: 99999
LOW:  00000
</code></pre>

<p>为了算下一个区间，我们用上一节的编码算法。首先要算区间长度。这两个寄存器的差是
<code>100000</code>，而非 <code>99999</code>。这是因为我们假设了高位后接了无穷多个 9，所以作完差要加一。接下来用上一节的公式算新的上界：</p>

<pre><code>high - low + high_range(symbol)
</code></pre>

<p>此时符号对应的区间上界是 <code>.30</code>，对应给我们新的上界值 30000。在把这个数存成上界的新值前，我们要把它减一，这次又要减还是因为我们想象中整数后面加了很多位。所以上界的新值是 <code>29999</code>。</p>

<p>用同样的方法计算下界，得到新值 <code>20000</code>。所以现在上下界看起来是这样：</p>

<pre><code>HIGH: 29999 (999…)
LOW:  20000 (000…)
</code></pre>

<p>这个节骨眼儿上，上下界最高位一样了。我们算法的性质决定上界和下界会继续不断接近而永不相遇。这意味着一旦它们最高位一样了，这一位就不会再变了。所以现在我们可以输出这一位了，它就是编码出来的数的第一位。输出时把上下界都左移一位，再在上界的低位移进一个 9。在本算法的 C 语言实现里，这些操作都是在二进制里做的。</p>

<p>这个过程持续进行，高位和低位不断接近，并把许多数字移出，写进最后的编码里。我们编码 &ldquo;BILL GATES&rdquo; 的过程如下：</p>

<pre><code>                     上界   下界    长度      累计输出

初状态               99999  00000   100000
编码 B (0.2-0.3)     29999  20000
移出 2               99999  00000   100000    .2
编码 I (0.5-0.6)     59999  50000             .2
移出 5               99999  00000   100000    .25
编码 L (0.6-0.8)     79999  60000   20000     .25
编码 L (0.6-0.8)     75999  72000             .25
移出 7               59999  20000   40000     .257
编码 空格 (0.0-0.1)  23999  20000             .257
移出 2               39999  00000   40000     .2572
编码 G (0.4-0.5)     19999  16000             .2572
移出 1               99999  60000   40000     .25721
编码 A (0.1-0.2)     67999  64000             .25721
移出 6               79999  40000   40000     .257216
编码 T (0.9-1.0)     79999  76000             .257216
移出 7               99999  60000   40000     .2572167
编码 E (0.3-0.4)     75999  72000             .2572167
移出 7               59999  20000   40000     .25721677
编码 S (0.8-0.9)     55999  52000             .25721677
移出 5               59999  20000             .257216775
移出 2                                        .2572167752
移出 0                                        .25721677520
</code></pre>

<p>注意在所有字母都处理完后，还得多移出两位，从上界或下界移都行，这样才能得到完整的结果。</p>

<h2>点儿背情况</h2>

<p>逐步编码一条信息时，这个方法做得很好。用双精度整数运算可以保证得到正确编码信息所需的精度。然而，特定情况下还是有丧失精度的风险。</p>

<p>考虑编码时上下界分别有一串 <code>0</code> 和 <code>9</code> 的情况，上界和下界会慢慢收敛，但它们的最高位不会一下子对上。比如，上下界可能是这样的：</p>

<pre><code>High: 70004    Low:  69995
</code></pre>

<p>这个肯綮儿上，算出来的区间长只有一位数，这意味着输出的字会没有足够的精度来保证编码的准确性。更不巧的是，再迭代几次，上下界看起来会像这样：</p>

<pre><code>High: 70000
Low:  69999
</code></pre>

<p>此时，这两个值就永久卡住了。上下界的距离很小，每再算一次都会返回相同的结果。然而，这两个值最高位永远不同，算法没法输出一位数，然后移位。看来走进死胡同了。</p>

<p>解决下溢的办法是防止情况变得这么糟。算法本来说的是“要是上下界的最高位相同，就移出去”。现在，它们若不同，但相邻，还得再考虑第二个情况。要是上下界的最高位只差一，我们看是不是上界的次高位是 <code>0</code>，而下界的次高位是 <code>9</code>。要是的话，这说明我们快要面对下溢了，得采取行动。</p>

<p>当下溢抬起它邪恶的头颅时，我们切掉这颗头，具体方法是做一个略微不同的移位操作。这次我们不移最高位，我们删掉上下界的次高位，并把剩下的数字移来补空儿。最高位保持不动。这时我们还得加个下溢计数器，记住我们扔掉了一位数，而且我们也不知道，这个数到头来会是 <code>0</code> 还是 <code>9</code>。这个操作大概是这样子的：</p>

<pre><code>            前        后
          -----     -----
上界:     40344     43449
下界:     39810     38100
下溢:     0         16000
</code></pre>

<p>这之后的每次计算，要是最高位仍不同，我们再查有没有下溢。要有，把它们移出去，再把计数器加一。</p>

<p>当最高位终于收敛到同一个值了，我们先把这个值输出。接下来，输出所有之前扔掉的“下溢”数字。下溢数字可能全是<code>9</code>或全是<code>0</code>，取决于上下界的最高位收敛到较大的数还是较小的数。在 C 实现里，下溢计数器记录了要输出多少零或一。</p>

<h2>解码</h2>

<p>在“理想”解码过程中，我们在整个数上操作。所以算法让我们“用符号的频率除编码出的数。”但是生产生活中，我们没法这么除，因为数可能有上亿字节长。实际上，跟编码过程一样，解码器只用 16 和 32 位整数就好了。</p>

<p>除了维护上下界这两个数，解码器还要维护第三个数。上下界这两个数的含义和编码器中的完全相同。第三个数叫原数，存着当前读进来要解码的数。原数总居上下界之间。当上下界靠得越来越近，新的移位就会发生，上下界又会离开原数。</p>

<p>解码器维护的上下界和编码器维护的上下界完全相应。它们在每个符号解出后都会更新，和编码器里的操作完全一样，而且值也一样。用同样的方法比较上下界的头几个数字，解码器清楚什么时候应该再向原数里移一位。下溢检查也同样做，每步都紧随编码器。</p>

<p>在理想算法里，可以直接找到哪个符号的概率范围包括了原数的当前值，从而定下当前的符号。不过在整数表示里，事情有那么点儿麻烦。算概率要乘的系数是上下界之差。所以区间此时并不是 <code>[0.0, 1.0)</code>，而是在两个 16 位整数之间。当前的概率由原数的当前值落在这个区间的什么位置决定。要得到当前符号的概率，你可以拿 <code>(high - low + 1)</code>
去除 <code>(value - low)</code>。</p>

<p>（译者：补个小例子：上界   下界   原数   剩下数    解码</p>

<pre><code>初状态               99999  00000  25721  .677520   
解码 B (0.2-0.3)     29999  20000  25721  .677520   B
移出 2               99999  00000  57216  .77520    B
解码 I (0.5-0.6)     59999  50000  57216  .77520    BI
移出 5               99999  00000  72167  .7520     BIL
解码 L (0.6-0.8)     79999  60000  72167  .7520     BIL
解码 L (0.6-0.8)     75999  72000  72167  .7520     BILL
移出 7               59999  20000  21677  .520   
解码 空格 (0.0-0.1)  23999  20000  21677  .520      BILL (末尾有个空格呦)
移出 2               39999  00000  16775  .20
解码 G (0.4-0.5)     19999  16000  16775  .20       BILL G
移出 1               99999  60000  67752  .0        BILL G
解码 A (0.1-0.2)     67999  64000  67752  .0        BILL GA
移出 6               79999  40000  77520  .0        BILL GA
解码 T (0.9-1.0)     79999  76000  77520  .0        BILL GAT
移出 7               99999  60000  75200  .0        BILL GAT
解码 E (0.3-0.4)     75999  72000  75200  .0        BILL GATE
移出 7               59999  20000  52000  .0        BILL GATE
解码 S (0.8-0.9)     55999  52000  52000  .0        BILL GATES
</code></pre>

<p>）</p>

<h2>牛肉在哪儿？</h2>

<p>那么到现在为止，编码过程看起来是挺有趣儿的，但是有一件事儿还不太明白，就是为什么算术编码比 Huffman 编码好。考虑一个概率稍稍不同的情况，答案就显然了。让我们考虑编码字符串 <code>"AAAAAAAA"</code>，且已知 <code>"A"</code> 的概率为 <code>0.90</code>。这意味着一个新来的字符有 <code>90%</code> 的机会是 <code>"A"</code>。我们建起我们的概率表，其中字母 <code>"A"</code> 占区间 <code>0.0</code>
到 <code>0.9</code>，消息结束符占区间 <code>0.9</code> 到 <code>1.0</code>。那么编码过程看起来会像这样：</p>

<pre><code>新字符      下界           上界
------   ----------     ----------
            0.0            1.0
  A         0.0            0.9
  A         0.0            0.81
  A         0.0           0.729
  A         0.0           0.6561
  A         0.0           0.59049
  A         0.0          0.531441
  A         0.0          0.4782969
 完了    0.43046721      0.4782969
</code></pre>

<p>既然我们知道了上下界，剩下的事情就只有找一个范围内的数来编码这条消息了。数
<code>"0.45"</code> 能唯一地解码成 <code>"AAAAAAA"</code>。这两个十进制位需要略少于 7 个二进制位来表示，于是我们用少于 8 个二进制位编码了八个字符！最优的 Huffman 编码至少也得用
9 位呢。</p>

<p>让我们把这一点推向极端，考虑一个测试，其中只出现两种字符。字符 <code>'0'</code> 有概率
<code>16382/16383</code>，而文件结束符出现概率是 <code>1/16383</code>。然后我创建了一个测试文件，里面填了 100,000 个 <code>'0'</code>。用我们的方法压缩，输出文件只有 3 字节长！用 Huffman
编码压出来最小也要 12,501 字节。这个例子显然是编出来的，但它确实表明在符号概率合适的情况下，算术编码可以做到比每字符 1 位更好的压缩率。</p>

<h2>代码</h2>

<p>列表 1 和 2 展示了 C 语言写的一个编码过程。它包括两部分：一个初始化过程和编码过程本身。编码器和解码器的代码最早在一篇名叫《数据压缩的算术编码》中发表，这篇文章发表在《ACM通讯》1987 年 2 月刊上，作者是 Ian H. Witten，Radford Neal 和
John Cleary，这些代码现在经作者允许发布在这里。我稍微改了改代码，更好地分开了程序的建模和编码部分。</p>

<p>上文讲的算法和列表 1-2 里的有两个主要差别。第一个是概率如何传递。上文的算法里，概率用 0.0 到 1.0 区间里的一对儿浮点数表示。每个符号拥有 [0, 1) 区间里自己的一个子区间。在这里展示的程序里，一个符号的概率表示起来略有不同。这里没有用两个浮点数，而用了两个整数来确定符号的区间，这两个整数是某个比例下数出来的。所以，还举“BILL GATES”的例子，字母 &ldquo;L&rdquo; 之前用上下界 0.60 和 0.80 来确定。在这里用的代码里，&#8221;L&#8221; 用上下计数 6 和 8 来确定，缩放比例是 10。原因是给这种数据建模的时候，这么做适应维护统计数据的方式。这两种方式是等价的，只是数整数儿能减少点儿工作。要注意的是一个字符实际上拥有从下界到上界但不包括上界的区间。</p>

<p>第二个差别是所有比较和位移操作都是在二进制下做的，不是十进制。之前的演示拿十进制做是为了让算法更好理解。这个算法在十进制下也能做，但是在大部分电脑上，取某几位和移位在十进制下都很难做。所以我们不比最高的两个数码，而比较最高的两个二进制位。</p>

<p>要用编码解码算法，还需要两样之前没讲的东西。第一是一组按位输入输出的函数。这些东西在列表 3 和 4 里，没加注释。建模代码负责维护每个字符的概率，以及做两种不同的变换。在编码过程中，建模代码拿来一个待编码的字符，把它转换成一个概率区间。概率区间用下计数、上计数和总区间来确定。在解码过程中，建模代码需要拿来一个从输入二进制位流里算出来的一个计数，并把它转换成输出字符。</p>

<h2>测试代码</h2>

<p>列表 3 展示了一个简短的测试程序。它实现了一个使用上述 <code>"BILL GATES"</code> 模型的压缩解压程序。加一点儿明智的打印语句能让你追踪这个程序究竟在做什么。</p>

<p><code>BILL.C</code> 有个非常简单的建模模块，对消息中可能出现的每个字符，模型里有它一个固定的概率。我还加了一个新的字符，空（或者叫字符串结束符），这样就可以直到什么时候停止解码消息了。</p>

<p><code>BILL.C</code> 编码一个任意的输入字符串，将结果写入一个名叫 <code>TEST.CMP</code> 的文件。它接着解码这条消息并打印到屏幕上来看看对不对。<code>BILL.C</code> 里还有些我们还没讨论过的建模函数。在编码过程中，要调用一个叫 <code>convert_int_to_symbol</code> 的函数。这个函数接受一个给定的输入字符，用当前的模型把它转换成一组下计数、上计数和缩放比例。因为我们 <code>BILL.C</code> 里的模型只是一组固定的概率，转换就只需要从表里查出概率。一旦上下计数和缩放比例定了，我们就可以去找编码器了。</p>

<p>在解码过程中，有两个与建模有关的函数。为了确定输入流里正在解码的是什么字符，必须从模型里找到当前的缩放比例。在我们的例子里，缩放比例（或者说计数的界）是固定的 11，因为我们的模型里一直只有 11 个计数。这个数传给了解码过程，解码过程根据这个比例反回给定符号的计数。接下来，调用一个叫 <code>convert_symbol_to_int</code> 的模型函数。该函数接受给出的计数，确定哪个字符与之相应。最后，再次调用解码器来从输入流中去掉这个字符。</p>

<h2>下一次</h2>

<p>只要你成功理解怎么用算术编码编解码符号，你就可以开始试着创建能给出极高压缩比的模型了。下个月的总结文章讨论些你可以用在自适应压缩上的方法。一个相当简单的统计模型程序就可以提供比人们尊敬的程序如 PKZIP 或 COMPRESS 更好的压缩比。</p>

<h2>参考文献</h2>

<p>如我之前所说，1987 年六月《ACM通讯》上的那篇文章提供了算术编码定义性的概述。这篇文章的绝大部分在《文本压缩》（Timothy C. Bell, John G. Cleary,及 Ian H. Witten 著）一书中重新印出。这本书给基于统计和字典的压缩技术都提供了优秀的概述。另一本好书是
James Storer 著《数据压缩》。</p>

<ul>
<li>Bell, Timothy C., Cleary, John G., Witten, Ian H, (1990) “Text Compression”, Prentice Hall, Englewood NJ</li>
<li>Nelson, Mark (1989) LZW Data Compression, Doctor Dobb’s Journal, October, pp 29-37</li>
<li>Storer, J.A., (1988) “Data Compression”, Computer Science Press, Rockville, MD</li>
<li><p>Witten, Ian H., Neal, Radford M., and Cleary, John G. (1987) Arithmetic Coding for Data Compression, Communications of the ACM, June, pp 520-540.</p></li>
<li><p> 列表1    <code>coder.h</code>    用算术编码所需的常数、定义和函数原型。这些定义是给要和
<code>coder.c</code> 中的算术编码部分交互的过程准备的。</p></li>
<li><p> 列表2    <code>coder.c</code>    包含完成一个符号的算术编码需要的代码。该模块里的所有过程都需要知道概率和符号计数的缩放比例到底是多少，以便编码。这些信息通常通过 <code>SYMBOL</code>
结构体传递。</p></li>
<li><p>列表3    <code>bitio.h</code>    包含用位流输入输出所需的函数原型。</p></li>
<li><p>列表4    <code>bitio.c</code>    包含一组算术编码压缩要用的面向位的输入输出过程。</p></li>
<li><p>列表5    <code>bill.c</code>    这个短的演示程序将用算术编码压缩来编码再解码仅包含短语 <code>"BILL GATES"</code>
中字母的字符串。它用着一个定阶数的，写死的概率表。</p></li>
</ul>


<p>第二部分 —— 用部分匹配预测(Prediction by Partial Matching, PPM)的统计模型</p>

<p>第一篇文章具体讲了算术编码是什么。一言蔽之，就是用最少的位编码符号的方法。而且不像 Huffman 编码，算术编码里每个符号不一定非要用整数位。为了用算术编码压缩数据，我们还需要统计模型。要有效压缩数据，统计模型得具备下面两个特点：</p>

<ul>
<li>模型要能准确预测输入数据流中符号的频率/概率。</li>
<li>模型生成的符号概率要偏离均匀分布 (uniform distribution)。</li>
</ul>


<p>本文会讨论达到这两个目标的几个办法，从而做到高比率压缩。</p>

<h2>模型</h2>

<p>准确预测输入数据中符号的概率，是算术编码的天性所求。这类编码的主旨是，当一个符号出现概率上升时，就减少编码它用的位数。所以如果字母 &ldquo;e&rdquo; 在输入数据中占 25%，那编码它只用 2 位。再比如字母 &ldquo;Z&rdquo; 仅占输入的 .1%，它有可能得花 10 位来编码。要是模型没能正确预测概率，我们就可能会花 10 位代表 &ldquo;e&#8221;，2 位代表 &#8220;Z&#8221;，导致数据膨胀而非压缩。</p>

<p>第二个条件是，模型提供的预测要偏离均匀分布。模型的预测对均匀分布的偏离越好，压缩比就越高。举个例子，我们可以构造一个模型，给 256 个符号赋均等的概率 1/256。这个模型的输出和原文件一样大，因为每个符号都恰好用 8 位来编码。只有正确地算出一个并不均匀的概率分布，编码用的位数才能减少，数据才能压缩。当然，在不均匀之前，概率分布首先要符合现实情况，这点在第一个条件中要求过了。</p>

<p>看起来，要是数据流给定了，某个特定的符号出现概率也随之确定，其实未必。所用模型不同，符号出现的概率变化会很大。比如说，压缩一个 C 源文件，换行符会有 1/40 的概率出现。这是扫描一遍文件后，拿换行符数除以总字符数算出来的。但是如果我们连同前一个字符考虑，概率就变了。这时，要是前一个字符是 &ldquo;}&#8221;，换行符的概率就升至 1/2。尽管两个模型算出的概率都是真的，但改进后的建模技术能提高压缩率。</p>

<h2>有限上下文模型和PPM</h2>

<p>本文中我要展示的建模方法是一种叫做有限上下文的方法。这种方法背后有个非常简单的主意：每个新来的符号的概率要基于它的上下文来算。我要举的所有例子里，所谓上下文都仅仅包括之前遇见的符号。模型的阶指上下文所包含的符号数。</p>

<p>我要实现的算法叫部分匹配预测，简称 PPM (Predication by Partial Matching)。PPM
程序一般基于有限上下文来给将来的字符预测概率。</p>

<p>最简单的有限上下文模型是 0 阶的。就是每个字符的频率都不依赖于之前遇见的字符。要实现这个模型，你只需为所有可能出现的字符做一张大表，表里记着每个字符在输入流中出现的概率。对 1 阶模型，因为要给每个可能的上下文分别记一组数，你就得维护 256 张不同的频率表了。以此类推，2 阶模型需要处理 65536 张不同的上下文表。</p>

<h2>自适应建模</h2>

<p>随着模型的阶升高，压缩比也要升高，看起来很合逻辑。举个例子，符号 &ldquo;u&rdquo; 在本文中出现的概率可能只有 5%，但要是上下文里前一个字符是 &ldquo;q&#8221;，概率就升至 95%。给字符预测出尽量高的频率可以减少编码需要用的位，而考虑更多的上下文能让我们作出更好的预测。</p>

<p>不幸得很，随模型阶数升高，它占用的内存成指数级上升。0 阶模型仅用 256 字节。一旦阶数涨到了 2 或 3，模型优化得再好也得占上百 KB 内存。</p>

<p>传统的压缩办法是先扫描一遍要压缩的符号，得到统计数据。之后再过一遍，编码数据。统计数据一般加在压缩后的数据前面，这样解码器工作时就有一份统计数据用了。当统计数据比原数据更占地的时候，这么做当然会出严重的问题。</p>

<p>解决办法是用自适应压缩。开始时，编码器和解码器统计模型的状态是相同的。编码器用当前模型编码一个字符后，会更新模型，记录这个字符的影响。解码器也一样，用当前模型解码一个字符后，更新模型。只要编码器和解码器更新模型的算法一样，就算编码器没有给解码器统计表，这整个过程也能进行得很完美。</p>

<p>自适应数据压缩是有些小的不足，开始压缩的时候它的统计数据并非最优。不过，考虑进传递统计数据的花费以后，自适应算法往往表现得比用静态统计模型要好。</p>

<p>自适应压缩真正疼的地方在于更新模型的代价。用算术编码而且要更新某个符号的计数时，有可能要影响到其它所有符号，因为更新前缀和。这样平均下来编解码单个符号就得做
128 次算术操作。</p>

<p>囿于内存和 CPU 时间的高昂开销，高阶自适应模型进入实用只是近 10 年的事。说起来挺讽刺的，磁盘空间和内存的开销降低了，压缩数据的花费也随之降低。而且开销还会继续降，日后我们会实现比现在更有效的压缩程序。</p>

<h2>一个简单例子</h2>

<p>列表 6 到 9 是一个 0 阶压缩程序的例子（1 到 5 在上个月文章里）。<code>COMP-1.C</code> 是压缩主程序，<code>EXPAND-1.C</code> 是解压主程序。<code>MODEL.H</code> 和 <code>MODEL-1.C</code> 里是一个 0 阶统计模型的实现。</p>

<p>用这么个定阶模型，压缩程序本身十分简单。程序只用待在一个循环里，从文本文件里读字符，把字符转成算术编码，编码字符，再更新模型。</p>

<p>理解这份代码怎么工作，关键在理解 <code>MODEL-1.C</code> 的建模部分。上月的文章里，我们看到了每个字符是怎么拥有整个概率区间 <code>[0, 1)</code> 的一个子区间的。 为了用整数算术来实现算数编码，每个字符所拥有的变成了 <code>[0, maximum_count)</code> 的一个整数子区间。
<code>MODEL-1.C</code> 里统计信息正好就是这么存的。</p>

<p><code>MODEL-1</code> 里，所有可能字符的出现次数存在一个整型数组 <code>totals[]</code> 里。对每个字符 <code>c</code>,
前一个字符出现次数存在 <code>totals[c]</code>，它的出现次数存在 <code>totals[c+1]</code> 里。 整个区间的上界存在 <code>totals[256]</code> 里。用算术编码器编码一个字符时，要把这三个参数传进去。</p>

<p>这么做的话查一个字符的计数非常直接。然而，在一个字符编码或解码后更新  <code>totals[]</code>
数组又是件麻烦事。为了更新字符 <code>c</code> 出现次数，<code>totals[]</code> 从 <code>c</code> 到 <code>256</code> 的每个数都得加一。这意味着平均情况下编码或解码每个字符要做 128 次加一。对 <code>MODEL-1.C</code> 这个演示程序来讲，这不是什么大问题，但是生产生活中，程序应该改得更高效。</p>

<p>一个降低加一操作数的办法是把最频繁出现的符号挪到数组头部。这样模型必须维护每个符号在 <code>totals[]</code> 数组中的位置，但好处是可以把加一操作数的复杂度降一阶。挪动元素算一个比较简单的优化。这个优化的一个好例子在《数据压缩的算术编码》（ACM通讯
1987 六月刊）里可以找到。这篇文章是 Ian H. Witten, Radford Neal, and John Cleary
写的，是关于算术编码非常好的信息来源，配有 C 源代码例解文本。</p>

<h2>性能</h2>

<p><code>COMP-1</code> 是一个小程序，占很少内存。因为它仅维护 0 阶统计信息。跟市面上的压缩程序比起来，它表现得并不太好。比如，压缩 C 源码时，<code>COMP-1</code> 一般用 5 位编码一个字节。尽管表现不出色，这样实现在内存有限的情况下还是有价值的。只要多一丁点儿内存，维护一个有序表就可以加速压缩和解压，同时不影响压缩比。</p>

<h2>改进</h2>

<p><code>COMP-1</code> 是一个合适的演示程序，但看起来可能不会让很多人兴奋。它比 0 阶 Huffman
编码程序稍好一点儿，但离 ARC 或 PKZIP 这类程序差得远。为了超过它们，我们得再对建模代码做些优化。<code>COMP-2.C</code>，<code>EXPAND-2.C</code> 和 <code>MODEL-2.C</code> 里可以找到所有优化的高潮，它们搭建了一个压缩程序和一个解压程序。</p>

<h2>高阶建模</h2>

<p>这里第一个优化就是提高模型阶数。算当前符号的概率时，一个 0 阶模型不考虑任何文本中之前的符号。要是考虑之前的符号，又叫上下文，我们可以更精确地预测新来地符号。</p>

<p>问题是，一个固定阶数的模型里，每个字符需要一个有限非零的概率，这样它出现的时候就能被编码了。因为我们对输入文本有什么样的统计特征没有先验的知识，一般我们只能在开始时给每个字符赋相等概率。这意味着，算上 EOF，-1 到 255 的每个字节都有 1/257 的概率在 &ldquo;REQ&rdquo; 这一符号串后出现。现在，就算某一行里字母 &lsquo;U&rsquo; 在这一串符号后出现了 10 次，它的概率也只增加到了 11/267。剩下的符号在概率表里占了宝贵的空间，尽管它们基本上不可能出现。</p>

<p>解决这个问题的办法是每个上下文里每个字符的初始频率都设成 0，然后搞一个碰到没有出现的字符时退到备用上下文里的方法。这通过发出一个特殊编码，一般叫逃逸码，来实现。对前面 &ldquo;REQ&rdquo; 的上下文，我们可以把逃逸码的出现次数设成 1，其它所有符号设成
0。第一次发现 &lsquo;U&rsquo; 跟在 &ldquo;REQ&rdquo; 后面的时候，我们发出来一个逃逸码，接着是 &lsquo;U&rsquo; 在另一个上下文里的编码。在此后模型更新的过程中，我们把 &lsquo;U&rsquo; 在 &ldquo;REQ&rdquo; 上下文里的出现次数加一，所以它现在拥有概率 1/2。下一次它出现时，它只需一位编码，出现次数越多，概率就越大，编码所需位数就越少。</p>

<p>那么当前的问题就成了：发出逃逸码后，拿什么当备用上下文？在 <code>MODEL-2</code> 里，要是
3 阶上下文生成了一个逃逸码，下一个尝试的是 2 阶上下文。意思是第一次用到上下文
&ldquo;REQ&rdquo; 且编码 &lsquo;U&rsquo; 时，会生成一个逃逸码。紧接着，<code>MODEL-2</code> 程序后退到 2 阶上下文，试图在上下文 &ldquo;EQ&rdquo; 中编码 &lsquo;U&#8217;。逐步后退，直到 0 阶。如果在 0 阶逃逸码仍然产生，我们退回到一个特别的 (-1) 阶上下文。-1 阶上下文初始时给每个符号出现次数都记为 1，并永不更新。所以这样就保证了编码每个字符。</p>

<p>用逃逸码技术要修改一下调用模型的程序。现在 <code>COMP-2.C</code> 需要在循环里编码字符：</p>

<pre><code>do {
     escaped - convert_int_to_symbol( c, &amp;s );
     encode_symbol( compressed_file, &amp;s );
} while ( escaped );
</code></pre>

<p>建模代码负责跟踪当前的阶数，在每一次发出逃逸码后降阶。更复杂的是模型要跟踪当前阶要用哪张上下文表。</p>

<h2>更新模型</h2>

<p>高阶模型算法要求除最高阶的一组上下文表外，还要保存从最低阶到最高阶的每一张表。因而即使我们在做 2 阶建模，程序里还会有一张 0 阶表，256 张 1 阶表和 65536 张 2 阶表。当一个新字符被编解码时，建模代码要在每阶上下文里更新一张表。在之前的例子里，&#8217;U&#8217; 跟着 &ldquo;REQ&#8221;，建模代码会在 3 阶 &#8220;REQ&rdquo; 表里更新 &lsquo;U&rsquo; 的计数，在 2 阶 &ldquo;EQ&rdquo; 表里更新 &lsquo;U&rsquo; 的计数，在 1 阶 &ldquo;Q&rdquo; 表里更新 &lsquo;U&rsquo; 的计数，以及 0 阶表里 &lsquo;U&rsquo; 的计数。</p>

<p>更新所有表的代码看起来是这个样子的：</p>

<pre><code>for ( order - 0 ; order &lt;- max_order ; order++ )
    update_model( order, symbol );
</code></pre>

<p>一个小修改可以加速压缩并提高压缩比。不要更新所有阶的模型，仅仅更新压缩这个字符时用过的模型就好啦。举个例子，如果 &lsquo;U&rsquo; 在 &ldquo;REQ&rdquo; 后出现且编码为 ESCAPE, &lsquo;U&#8217;，我们只会在 &ldquo;REQ&rdquo; 和 &ldquo;EQ&rdquo; 模型里更新计数器，因为表 &ldquo;EQ&rdquo; 里找到 &#8216;U&rsquo; 的出现次数了。&#8221;R&#8221;
和 &ldquo;&rdquo; 表 都没有受到影响。</p>

<p>这种对全部更新的修改叫做更新排除，因为它把没用到的低阶模型排除在了更新范围外。这样通常会给出一点小但可见的压缩率优化。更新排除的工作原理是，注意到事实如果一些符号在高阶模型里出现得频繁，它们就不会频繁出现在低阶模型里，这意味着我们不应该在低阶模型里更新它们。更改后的更新排除代码看来像这样：</p>

<pre><code>for ( order-encoding_order ; order &lt;- max_order ; order++ )
    update_model( order, symbol );
</code></pre>

<h2>逃逸频率</h2>

<p>刚开始编码文本流时，我们会发出很多逃逸码。这种情况下，编码逃逸码用到的位数可能极大地影响我们达到的压缩比，尤其是小文件。在 <code>MODEL-2A.C</code> 里，我们把逃逸码计数设为 1 并留在那里不动， 不管其余上下文表的状态。这么做等价于 Bell, Cleary
和 Witten 所谓的方案A。方案B把逃逸字符的频率设成当前上下文表中出现的字符数。这样，如果截至当前已经见到了 11 个不同的字符，逃逸符的出现次数会被设成11，不管它真正的出现次数是多少。</p>

<p>方案 A 和方案 B 工作得都不错。<code>MODEL-2.C</code> 里的代码稍稍改动一下就可以支持任一个。也许这两个方案最好的地方是它们计算量都不大。用方案 A 的话，更新统计表时就可以几乎无视逃逸字符的存在。</p>

<p>在 <code>MODEL-2.C</code> 里，我实现了一个稍微复杂一点儿的逃逸计数算法。计算逃逸概率时，这个算法会考虑三种不同因素。首先，有一件事看起来有道理，就是说上下文表里的字符多了，逃逸频率会降低。字符最多 256 个，当 256 个字符都出现后，逃逸频率即降至 0。</p>

<p>其次，这个算法想要考虑表的无序性。它用（所有字符中）最多的出现次数除以平均出现次数之商来算无序性。这个比值越高，表就越不无序。一个好的例子是 &ldquo;REQ&rdquo; 表。里面可能只有 3 个字符出现过：&#8217;U&#8217; 50 次，&#8217;u&#8217; 10 次以及 &lsquo;.&rsquo; 3 次。&#8217;U&#8217; 出现次数 50 和平均次数 21 的比值很高。这说明要是猜下一个字符是 &lsquo;U&#8217;，猜中的概率很大，所以逃逸概率应该低一些。在一个最大出现次数是 10，而平均为 8的表里，字符分布可能比较不平均，逃逸概率应该高一些。</p>

<p>决定逃逸概率时，最后考虑的因素是直接数表中字符总出现次数来决定逃逸概率。随符号数增加，表的可预测性上升，使得逃逸概率下降。</p>

<p>我用来计算逃逸字符出现次数的公式如下：</p>

<pre><code>count - (256 - number of symbols seen)*number of symbols seen
count - count /(256 * the highest symbol count)
if count is less than 1
    count - 1
</code></pre>

<p>公式里漏掉了表中字符总出现次数。不过计算中已隐含了这条信息，因为逃逸概率是逃逸字符出现数除以字符总出现次数。字符总出现次数会自动把逃逸次数缩成一个概率。</p>

<h2>计分版</h2>

<p>用高阶建模技术时，有一个提高压缩效率的额外优化。第一次用高阶上下文压缩一个字符时，我们可能生成字符的编码或者生成逃逸码。如果我们生成逃逸码，那一定是这个符号在这个上下文中从未出现过，所以我们这里它的出现次数是 0。不过仅生成逃逸符我们也还是能够获得一些关于这个字符的信息。我们回去看逃出来的上下文，生成一个与待编码字符不同的符号列表。用低阶模型编码时，这些符号的出现次数可以暂时置 0。编码完这个字符后，出现次数会重置为它本来的值。</p>

<p>下面有个例子。如果当前上下文是 <code>"HAC"</code>，下一个符号是 <code>'K'</code>，我们会用到以下表格编码 <code>'K'</code>。不用计分版的话，<code>"HAC"</code> 上下文以 1/6 概率生成逃逸符。<code>"AC"</code> 上下文以 1/7 概率生成逃逸符。<code>"C"</code>上下文以 1/40 概率生成逃逸符，最终 <code>""</code> 上下文以
1/73 概率生成字符 <code>'K'</code>。</p>

<pre><code>   ""            "C"            "AC"           "HAC"
--------       --------       --------        -------
ESC   1        ESC   1        ESC    1        ESC   1
'K'   1        'H'   20       'C'    5        'E'   3
'E'   40       'T'   11       'H'    2        'L'   1
'I'   22       'L'   5                        'C'   1
'A'   9        'A'   3
</code></pre>

<p>如果用记分板来排除之前见过的字符，我们能对这些概率做显著优化。第一次从
<code>"HAC"</code> 中编码逃逸码没受影响，因为之前没有出现过字符。然而，<code>"AC"</code> 中逃逸符就可以不算字符 <code>'C'</code> 了，得到概率 1/3。<code>"C"</code> 逃逸码的概率计算中排除了
<code>'H'</code> 和 <code>'A'</code> 的计数，将算出的概率从 1/40 提升至 1/17。最终，<code>""</code> 上下文排除掉了 <code>'E'</code> 和 <code>'A'</code> 的计数，把概率从 1/73 降到了 1/24。这导致编码这些字符用的位数从 14.9 降到了 12.9，节约很明显。</p>

<p>做记分板总能提高一些压缩率，而且永远不会让事情变糟。记分板的主要问题是，每次访问低阶上下文表时，所有概率都得重算一遍。导致编码文本所用的 CPU 时间大幅上升。不过 <code>MODEL-2.C</code> 中留用了记分板，以显示用它的时候压缩文本有什么可能的获益。</p>

<h2>数据结构</h2>

<p>所有对最基本的统计模型的优化都假定高阶模型可以在目标机器上实现。阶数增高带来的是内存问题。<code>MODEL-1</code> 中的 0-阶模型里，概率表占 516 字节内存。如果我们在 1-阶模型里用相同的数据结构，占用的内存会激增至 133 K字节，也许尚可接受。但是到了 2-阶模型，统计数据占的内存会增至 34M字节！这样我们要是想试用高于二阶的模型，我们就得重新设计用来计数的数据结构。</p>

<p>为了节省内存空间，我们要重新设计上下文表。在 <code>MODEL-1</code> 中，这个表能多简单就多简单，每个符号直接在上下界数组充当下标。到了 1-阶模型，可以先在上下文表的数组中取下标得到相应的上下文表，完了再取一次下标到要查的字符，像这样：</p>

<pre><code>low_count = totals[last_char][current_char];
high_count = totals[last_char][current_char+1];
range = totals[last_char][256];
</code></pre>

<p>这样很方便，但特别特别浪费。首先，就算没用的上下文表也得分配整个空间。第二，在这些表里，每个字符都分了空间，不管它们是否出现过。这两个因素导致高阶模型浪费了大量的空间。</p>

<p>第一个问题（给没用过的上下文也预留了空间）的解是，把上下文表组织成一棵树。开头我们把 0-阶上下文表存在一个已知的位置，然后用它存到 1-阶上下文表的指针。
1-阶上下文表存着它们的统计信息和到 2-阶上下文表的指针。这样下去一直到这棵上下文树的“叶子”，即 n-阶上下文表，且不含到更高阶的指针。</p>

<p>通过使用树结构，我们可以把没用过的指针节点设成 <code>NULL</code> 指针直到一个上下文出现了。一旦遇到了这样一个上下文，我们建一张表，并把这张表接到树的父节点下。</p>

<p>第二个问题是，每次创建一个新的上下文后都得创建一张有 256 个计数器的表。事实上，最高阶上下文常常只有很少几个符号，所以如果只给出现过的符号分配内存，我们可以省下大量空间。</p>

<p>实现这些修改后，我手头上的数据结构长这样：</p>

<pre><code>typedef struct {
                unsigned char symbol;
                unsigned char counts;
               } STATS;

typedef struct {
                struct context *next;
               } LINKS;

typedef struct context {
                         int max_index;
                         STATS *stats;
                         LINKS *links;
                         struct context *lesser_context;
                       } CONTEXT;
</code></pre>

<p>新的 <code>CONTEXT</code> 结构体有四个主要的元素。第一个是计数器 <code>max_index</code>。
<code>max_index</code> 数着某个上下文表中当前记录了多少个元素。当一个表刚建出来的时候，里面没有记录任何元素，此时 <code>max_index</code> 值为 <code>-1</code>。定义了所有元素的表中
<code>max_index</code> 为 <code>255</code>。<code>max_index</code> 还表示 <code>*stats</code> 和 <code>*links</code> 指向的数组中已分配的元素个数。<code>stats</code> 是个结构体数组，每个结构体包含一个符号和这个符号的计数。如果 <code>CONTEXT</code> 表不是最高阶表，它还有个 <code>links</code> 数组。对每个 <code>stats</code> 数组中的符号，<code>links</code> 数组里有一个指向高一阶 <code>CONTEXT</code> 表的指针。</p>

<p>图一展示了一个 <code>CONTEXT</code> 表树。展示的表就是刚刚输完文本 <code>"ABCABDABE"</code>，且最高保存 3-阶统计信息得到的。仅仅 9 个输入符号就已经生成了一个相当复杂的数据结构，但它仍比一个含有数以数组计的数组的数据结构小许多个数量级。</p>

<pre><code>      Order 0            Order 1             Order 2            Order 3
|------------------|------------------|-------------------|------------------|
|Context: ""       |Context: "A"      |Context: "AB"      |Context: "ABC"    |
| Lesser: NULL     | Lesser: ""       | Lesser: "B"       | Lesser: "BC"     |
| Symbol Count Link| Symbol Count Link| Symbol Count Link | Symbol Count Link|
|   A      3   "A" |   B      3   "AB"|   C      1   "ABC"|   A      1   NULL|
|   B      3   "B" |------------------|   D      1   "ABD"|------------------|
|   C      1   "C" |Context: "B"      |   E      1   "ABE"|Context: "BCA"    |
|   D      1   "D" | Lesser: ""       |-------------------| Lesser: "CA"     |
|   E      1   "E" | Symbol Count Link|Context: "BC"      | Symbol Count Link|
|------------------|   C      1   "BC"| Lesser: "C"       |   B      1   NULL|
                   |   D      1   "BD"| Symbol Count Link |------------------|
                   |   E      1   "BE"|   A      1   "BCA"|Context: "CAB"    |
                   |------------------|-------------------| Lesser: "AB"     |
                   |Context: "C"      |Context: "CA"      | Symbol Count Link|
                   | Lesser: ""       | Lesser: "A"       |   D      1   NULL|
                   | Symbol Count Link| Symbol Count Link |------------------|
                   |   A      1   "CA"|   B      1   "CAB"|Context: "ABD"    |
                   |------------------|-------------------| Lesser: "BD"     |
                   |Context: "D"      |Context: "BD"      | Symbol Count Link|
                   | Lesser: ""       | Lesser: "D"       |   A      1   NULL|
                   | Symbol Count Link| Symbol Count Link |------------------|
                   |   A      1   "DA"|   A      1   "BDA"|Context: "BDA"    |
                   |------------------|-------------------| Lesser: "DA"     |
                   |Context: "E"      |Context: "BE"      | Symbol Count Link|
                   | Lesser: ""       | Lesser: "E"       |   B      1   NULL|
                   | Symbol Count Link| Symbol Count Link |------------------|
                   |------------------|-------------------|Context: "DAB"    |
                                                          | Lesser: "AB"     |
                                                          | Symbol Count Link|
                                                          |   E      1   NULL|
                                                          |------------------|
                                                          |Context: "ABE"    |
                                                          | Lesser: "BE"     |
                                                          | Symbol Count Link|
                                                          |------------------|


                               "ABCABDABE"
                                  图 1
</code></pre>

<p>这个结构体里有一个元素我没有解释，即 <code>lesser_context</code> 指针。用了高阶模型后，对这个指针的需求就是显然的事情了。如果我的建模代码试图定位一个 3-阶上下文表，首先它得遍历一遍 0-阶符号表找到第一个匹配的符号，然后找 1-阶符号表，以此类推。如果低阶符号表相对比较满，这个过程会很花时间。更不好的是，每生成一个逃逸码后，查低阶上下文的过程要不断重复。这些查询会无节制地花掉 CPU 时间。</p>

<p>这个问题的解是给每个表配一个指向恰低一阶的表的指针。举个例子，上下文表 <code>"ABC"</code>
的回溯指针指向 <code>"BC"</code>，而 <code>"BC"</code> 的回溯指针指向 <code>"C"</code>，<code>"C"</code> 的回溯指针指向 <code>""</code>，即空表。然后，建模代码总是维护着指向当前最高阶上下文的指针。如此，找到某个 i-阶上下文就只是做 <code>(n-i)</code> 次指针运算的事儿了。</p>

<p>举个例子，在图1所示的表里，如果下一个进来的符号是 <code>'X'</code> ，并且当前上下文是  <code>"ABE"</code>。不用回溯指针的话，为了符号 <code>'X'</code>，我要查 3, 2, 1 以及 0-阶表。总共 <code>3</code>  次查表，<code>15</code> 次符号比较。用回溯指针就消掉了所有符号比较，我只需做 <code>3</code> 次查表。</p>

<p>维护回溯指针的工作是在模型更新时做的。比如在图 1 所示的上下文树里，在 <code>"ABE"</code> 后面加一个 <code>'X'</code> ，建模代码得对每阶上下文执行一次遍历。代码在 <code>MODEL-2.C</code> 的
<code>add_character_to_model()</code> 过程里。每建一个新表，回溯指针都得及时正确地建出来，而这需要在设计更新过程时小心一点儿。</p>

<h2>最后一笔：表 -1 和 -2</h2>

<p><code>MODEL-2.C</code> 上下文树的最后一笔是加上两个特殊的表。-1-阶表之前讨论过了。这个表里每个元素都有固定的概率。当有一个符号在每个高阶模型里都没有出现过的时候，我们保证它会在 -1-阶模型中出现。这是最后的救命表。因为它保证不管什么符号都能提供一个编码，我们不更新这张表，因而里面每个符号都有个固定的概率。</p>

<p>此外，我加了一个特殊的 -2-阶表，用来从编码器给解码器传递一些控制信息。举个例子，编码器可以传一个 -1 代表文件结束。由于正常的符号都用 0 到 255 的无符号值表示，模型代码会把负数认成一个特殊符号，它会一直生成逃逸码直到 -2阶表。建模代码还能检测到因为它是一个负数，当调用 <code>update_model ()</code> 时，这个符号就这么着被忽略了。</p>

<h2>模型清洗</h2>

<p>-2 阶模型让我可以从编码器到解码器传另一个控制码。它叫清洗码，告诉解码器从模型里洗掉所有统计信息。我在压缩比开始下滑的时候执行清洗操作。比率是可调的，我用 <code>90%</code> 做为我能忍的最差压缩比。当超过这个比例后，我清洗模型，办法是把所有计数除以二。这样会给新来的统计信息更大的权重，如此应该有助于改进模型的压缩表现。</p>

<p>事实上只要输入符号的特性发生很大改变，模型也许就该清洗了。举个例子，比如这个程序正在压缩一个可执行文件，压缩代码段得到的统计信息在压缩数据段的时候也许没有任何价值。不幸的是，不好写个算法检测输入数据“性质的改变”。</p>

<h2>实现</h2>

<p>就算用了这么拜占庭的数据结构，基于 <code>MODEL-2.C</code> 的压缩和解压程序仍须巨量内存。在限 640K 的 DOS 机上，这些程序只能实现 1-阶模型，或者 2-阶如果文本的冗余比较大。</p>

<p>为了观察二进制文件上用高阶模型得到的压缩比，我们的程序有三种选择。首先可以用
Zortech C 编译它们，并用 EMS __handle 指针。其次，他们可以用一个 DOS Extender，比如 Rational Systems 16/M 生成。第三，它们可以在支持虚拟内存的机器上编译，比如
VMS。这里发布的代码试图兼容这三个选项。</p>

<p>我发现用上 EMS 多出的一兆字节，我可以在我的 PC 上用 3-阶模型压缩事实上任何 ASCII
文件。有些二进制文件需要更多内存。我的 Xenix 系统处理 3-阶压缩无压力，且在速度方面表现最好。我还试了 DOS Extender。因为未知原因，我用 Lattice C 286 和
Microsoft C + DOS 16/M 测出来比 Zortech 的 EMS 代码慢好多，尽管逻辑上来讲不对。这也不是优化的问题，因为在 640K 下 Lattice 和 Microsoft 的实现比 Zortech 跑得快。用
Lattice C/286 和 Zortech 2.06 生成的代码在 DDJ 列表服务里可以获得。Rational Systems 16/M
协议要求  <code>royalty payments</code>，所以这个代码不能分发。</p>

<h2>测试和比较：CHURN</h2>

<p>为了测试压缩程序，我创造了一个叫 CHURN 的一般用途测试程序。CHURN 就是翻一遍一个目录和它的所有子目录，压缩完了解压它找到的所有文件。在每次压缩解压后，比较原始输入和最后输出来保证它们一模一样。然后把时间和空间的压缩统计加到累计总值里，然后程序继续。程序结束时，它打印出这次的累计统计值用来分析。（空间所限，CHURN 和它的 Unix 变种 CHURNX 没写在这里。它们都可通过 DDJ 列表服务获得）。</p>

<p>CHURN 用单个参数调用，参数是放着测试数据的目录名。我一般把 CHURN 的输出定向到一个日志文件里这样可以以后分析，如此：</p>

<pre><code>     churn c:\textdata &gt; textdtat.log
</code></pre>

<p>压缩程序会往 <code>stderr</code> 写一些信息，这样压缩的时候这些信息会显示在屏幕上。注意在测试不同程序的时候，<code>CHURN.C</code> 需要经常修改，看是调用 <code>spawn()</code> 还是 <code>system()</code>。</p>

<p>下面表 1 里展示的是用两种输入测试多种压缩程序得到的结果。第一个例子，TEXTDATA，里面基本上是 1 兆字节的文本文件，有源代码、字处理文件和文档。第二个例子，BINDATA，里面基本上是 1 兆字节随便找的文件，有可执行文件、数据库文件、二进制图像等。</p>

<p>我用七个不同的压缩程序在这两个数据集上都跑了一遍。COMPRESS 是一个 Unix 系统上广泛应用的 16 位 LZW 实现。PC 上的 16 位实现要占用约 500K 内存。LZHUF 是xx晴康写的一个带自适应 Huffman 编码的 LZSS 程序，之后被 Paul Edwards 改了并发到 Fidonet 上。它和 LHARC 用的压缩办法本质上一样。最后，PKWare（Glendale, Wisconsin） 的商业产品
PKZIP 1.10 也在这些数据上试过了。PKZIP 用了一个私有的基于字典的方法，详见程序文档。</p>

<p>结果显示完全优化的 <code>MODEL-2</code> 算法在测试用的数据集上能提供最好的压缩比，而且在文本数据上表现尤为出色。二进制数据看上去并不是那么适合统计分析，在二进制数据上字典方法和 <code>MODEL-2</code> 的表现差不多。</p>

<pre><code>TEXTDATA - Total input bytes:    987070    Text data
Model-1   Fixed context order-0.          548218 bytes    4.44 bits/byte
Model-1A  Fixed context order-1.          437277 bytes    3.54 bits/byte
Model-2A  Highest context, max order-1.   381703 bytes    3.09 bits/byte
COMPRESS  Unix 16 bit LZW implementation. 351446 bytes    2.85 bits/byte
LZHUF     Sliding window dictionary       313541 bytes    2.54 bits/byte
PKZIP     Proprietary dictionary based.   292232 bytes    2.37 bits/byte
Model-2   Highest context, max order-3.   239327 bytes    1.94 bits/byte

BINDATA - Total input bytes:   989917     Binary data
Model-1   Fixed context order-0.          736785 bytes    5.95 bits/byte
COMPRESS  Unix 16 bit LZW implementation. 662692 bytes    5.35 bits/byte
Model-1A  Fixed context order-1.          660260 bytes    5.34 bits/byte
Model-2A  Highest context, max order-1.   641161 bytes    5.18 bits/byte
PKZIP     Proprietary dictionary based.   503827 bytes    4.06 bits/byte
LZHUF     Sliding window dictionary       503224 bytes    4.06 bits/byte
Model-2   Highest context, max order-3.   500055 bytes    4.03 bits/byte
</code></pre>

<h2>结论</h2>

<p>就压缩比而言，这些测试显示统计模型至少比字典方法表现不差。然而，这些程序现在有点不实用因为它们要占很多资源。<code>MODEL-2</code> 相当慢，大概每秒压缩 1 K字节，并且需要大量内存来启用高阶模型。然而，随着内存变便宜和处理器变快，这里展示的方法会变得可行。现在，它们有可能会用在存储或传输开销极高的情景。</p>

<p>0 阶自适应模型加算术编码现在可用在要求极低内存占用的场合。压缩比也许不比更复杂的模型好，但内存占用是最少的。</p>

<h2>强化</h2>

<p>这些算法的表现可以在我们讨论到的实现之外得到显著提升。第一个可以改进的地方是内存管理。现在，当程序用尽内存时，它们退出。更合理的办法是预设统计信息允许占用的空间，当统计信息占满了这个空间，程序停止更新表，用现有信息压缩。这可能意味着得自己写内存管理函数而不是用 C 运行时库里的。</p>

<p>另一个潜在的改进在上下文表的树结构里。用哈希定位表可能会快一点儿，同时减少内存需求。上下文表本身也可以改进。当一个表中超过 50% 的元素都出现过了，就可以换一种数据结构，里面各符号的计数存在一个数组里。这样既可以快速查找，又可以省内存。</p>

<p>最后，自适应地改变用的模型阶数也可能是有价值的。当用 3-阶模型时，在填充统计表的过程中，文本的开头部分会产生大量逃逸码。开始阶段只用 0-阶模型，同时维护着 3-阶模型。当表填得差不多了，可以采用高一阶模型一直到最高阶。</p>

<ul>
<li><p>列表6    <code>comp-1.c</code>    0-阶固定上下文压缩的驱动程序。使用 <code>BILL.C</code> 程序的压缩模式，即从文件中读入符号，把符号转换成一个区间，然后编码它们。程序里用的 0-阶模型代码可以在列表 9 <code>model-1.c</code> 里找到。</p></li>
<li><p>列表7    <code>expand-1.c</code>    0-阶固定上下文解压缩的驱动程序。</p></li>
<li><p>列表8    <code>model.h</code>    与 <code>model-1.c</code> 和 <code>model-2.c</code> 中的模型代码交互所需的函数原型和 <code>extern</code> 变量声明。</p></li>
<li><p>列表9    <code>model-1.c</code>    0-阶固定上下文压缩程序里的模型模块。</p></li>
<li><p>列表10    <code>comp-2.c</code>    可变有限阶数上下文压缩的驱动程序。最高阶数由命令行参数决定。该版本也监控压缩率，且在最近（最后 256 个字符内）的压缩率升至 90% 或更高是清洗模型。</p></li>
<li><p>列表11    <code>expand-2.c</code>    可变有限阶数上下文解压缩的驱动程序。最高阶数由命令行参数决定。该版本可响应位流中由压缩器插入的 <code>FLUSH</code> 编码。</p></li>
<li><p>列表12    <code>model-2.c</code>    包含 <code>comp-2.c</code> 和 <code>expand-2.c</code> 里用到的所有模型函数。该模型模块维护从 <code>0</code> 直到 <code>max_order</code>（默认<code>3</code>） 的各阶上下文。</p></li>
<li><p>列表13    <code>churn.c</code>    测试压缩解压程序正确性、速度及压缩比的小工具。用一个参数调用，
<code>CHURN</code> 会压缩解压指定路径下的所有文件，检查压缩比和正确性。你要觉得你的新算法正常工作了，<code>CHURN</code> 会是一个你拿来跑一晚上的好程序。</p></li>
<li><p>列表14    <code>churnx.c</code>    UNIX 系统可用的 <code>churn</code> 程序。</p></li>
</ul>

</div>

</article>

	
	<div class="share">
	  <ul>
	  <li>
  <a href="https://twitter.com/intent/tweet?text=[翻译]算术编码+统计模型=数据压缩 by @&url=http://zhzhzoo.github.io/blog/2014/02/fan-yi-suan-zhu-bian-ma-plus-tong-ji-mo-xing-equals-shu-ju-ya-suo/" title="Share [翻译]算术编码+统计模型=数据压缩 on Twitter">
    <img src="/images/social/twitter.png" />
  </a>
</li>

	  <li>
  <a href="https://www.facebook.com/sharer.php?u=http://zhzhzoo.github.io/blog/2014/02/fan-yi-suan-zhu-bian-ma-plus-tong-ji-mo-xing-equals-shu-ju-ya-suo/" title="Share [翻译]算术编码+统计模型=数据压缩 on Facebook">
    <img src="/images/social/facebook.png" />
  </a>
</li>

	  <!--   <li>
  <a href="https://plus.google.com/share?url=http://zhzhzoo.github.io/blog/2014/02/fan-yi-suan-zhu-bian-ma-plus-tong-ji-mo-xing-equals-shu-ju-ya-suo/" title="Share [翻译]算术编码+统计模型=数据压缩 on Google Plus">
    <img src="/images/social/google.png" />
  </a>
</li>
 -->
	  </ul>
	</div>




<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>
</div>
	<footer id="footer" class="inner"><br>
<br>
<br>
<br>
&copy; 2014

    zhzhzoo

</footer>
	<script src="/javascripts/slash.js"></script>
<!--<script src="/javascripts/hyphenator.js"></script>-->


<script type="text/javascript">
      var disqus_shortname = 'dispnil';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://zhzhzoo.github.io/blog/2014/02/fan-yi-suan-zhu-bian-ma-plus-tong-ji-mo-xing-equals-shu-ju-ya-suo/';
        var disqus_url = 'http://zhzhzoo.github.io/blog/2014/02/fan-yi-suan-zhu-bian-ma-plus-tong-ji-mo-xing-equals-shu-ju-ya-suo/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>






</body>
<script>
  $(document).ready(function() {
  // Make images center
  $('p:has(img)').css('text-align', 'center');
  });
</script>
</html>
